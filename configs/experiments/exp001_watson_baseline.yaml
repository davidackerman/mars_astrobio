# Experiment 001: WATSON Baseline with ResNet-50
#
# First baseline experiment to establish performance on WATSON texture classification.
# Uses pretrained ResNet-50 with transfer learning from ImageNet.

experiment:
  name: "exp001_watson_baseline_resnet50"
  description: "Baseline texture classification with pretrained ResNet-50"
  tags: ["baseline", "resnet50", "watson", "texture"]
  seed: 42

# Data configuration
data:
  root: "data/processed/watson_texture_v1"

  # Data loader settings
  batch_size: 32
  num_workers: 4
  pin_memory: true

  # Image size
  image_size: 224  # ResNet-50 standard input

  # Train transforms
  train_transform:
    - RandomResizedCrop:
        size: 224
        scale: [0.8, 1.0]
    - RandomHorizontalFlip:
        p: 0.5
    - RandomVerticalFlip:
        p: 0.5
    - ColorJitter:
        brightness: 0.2
        contrast: 0.2
        saturation: 0.1
        hue: 0.05
    - ToTensor: {}
    - Normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

  # Val/test transforms
  val_transform:
    - Resize:
        size: 256
    - CenterCrop:
        size: 224
    - ToTensor: {}
    - Normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

# Model configuration
model:
  name: "texture_cnn"

  # Architecture
  backbone: "resnet50"
  pretrained: true
  freeze_layers: 0  # 0 = train all layers

  # Classification head
  num_classes: 5  # laminations, porosity, halos, alteration, normal
  dropout: 0.5
  hidden_dims: [2048, 512]

# Training configuration
training:
  # Optimizer
  optimizer:
    type: "AdamW"
    lr: 0.0001
    weight_decay: 0.01
    betas: [0.9, 0.999]

  # Learning rate scheduler
  scheduler:
    type: "CosineAnnealingLR"
    T_max: 50
    eta_min: 0.000001

  # Loss function
  loss:
    type: "CrossEntropyLoss"
    label_smoothing: 0.1  # Regularization
    # Use class weights for imbalanced data (computed automatically)
    use_class_weights: true

  # Training parameters
  epochs: 50
  gradient_clip: 1.0
  mixed_precision: true
  accumulation_steps: 1

  # Callbacks
  callbacks:
    # Early stopping
    early_stopping:
      enabled: true
      patience: 10
      monitor: "val_loss"
      mode: "min"

    # Model checkpointing
    model_checkpoint:
      enabled: true
      save_top_k: 3
      monitor: "val_f1"
      mode: "max"
      save_last: true
      dirpath: "models/checkpoints/exp001"

  # Logging
  logging:
    tensorboard: true
    log_interval: 10
    log_images: true  # Log sample predictions

# Evaluation
evaluation:
  # Metrics to track
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - confusion_matrix

  # Compute per-class metrics
  per_class_metrics: true

  # Test-time augmentation
  tta:
    enabled: false  # Disable for baseline

# Reproducibility
seed: 42
deterministic: true
benchmark: false

# Expected performance (baseline targets)
targets:
  val_accuracy: 0.75  # Target 75% validation accuracy
  val_f1: 0.70  # Target 70% F1 score
  notes: |
    Baseline experiment to establish performance floor.
    Expected challenges:
    - Limited training data initially
    - Class imbalance (rare biosignature textures)
    - Transfer from natural Earth images to Mars textures

    Success criteria:
    - Model performs better than random (20% accuracy)
    - Model learns some texture patterns (>60% accuracy)
    - Can identify at least some biosignature examples
